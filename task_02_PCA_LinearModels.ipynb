{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute this for the 1st time\n",
    "# !pip install -r example-requirements.txt \n",
    "\n",
    "# download file \n",
    "# !wget http://helon.usuarios.rdc.puc-rio.br/data/data3SS2009.mat # linux\n",
    "\n",
    "# and place in folder data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from os import getcwd\n",
    "from os.path import join \n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = join(getcwd(),'data','data3SS2009.mat')\n",
    "mat_contents = sio.loadmat(fname)\n",
    "\n",
    "print (type(mat_contents))\n",
    "mat_contents.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating parameters and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('dataset: ',mat_contents['dataset'].shape)\n",
    "print ('labels: ',mat_contents['labels'].shape)\n",
    "\n",
    "dataset = mat_contents['dataset']\n",
    "N, Chno, Nc = dataset.shape\n",
    "# N: number of samples\n",
    "# Chno: number of channels\n",
    "# Nc: number of cases, experiments\n",
    "\n",
    "y = mat_contents['labels'].reshape(Nc)\n",
    "print ('y: ',len(y))\n",
    "y[::50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new label y2 considering the classification as: (in case we use it as a supervised label instead of the 17 classes)\n",
    "\n",
    "y=(1..9) - undamaged, y2 = -1\n",
    "\n",
    "y=(10..17) - damaged, y2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = []\n",
    "for label in y:\n",
    "    if label in range(1,10):\n",
    "        y2.append(-1)\n",
    "    else: y2.append(1)\n",
    "y2 = np.array(y2)\n",
    "ax = sns.scatterplot(y,y2)\n",
    "ax.set_title('y2 (damaged or undamaged) vs y (1 to 17)')\n",
    "ax.set_xlabel('y1')\n",
    "ax.set_ylabel('y2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Specific Channel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ch1 = dataset[:,0,:] # célula de carga: força do shaker\n",
    "Ch2 = dataset[:,1,:] # acelerômetro: base\n",
    "Ch3 = dataset[:,2,:] # acelerômetro: 1o andar\n",
    "Ch4 = dataset[:,3,:] # acelerômetro: 2o andar\n",
    "Ch5 = dataset[:,4,:] # acelerômetro: 3o andar\n",
    "\n",
    "Ts = 3.125 * 1e-3 # sampling time\n",
    "time = (np.linspace(1,N,N) - 1) * Ts\n",
    "\n",
    "for case in np.array([0,790]):\n",
    "    plt.subplots()\n",
    "    plt.subplot(211)\n",
    "    plt.title('case %d, label %d' % (case, y[case]))\n",
    "    plt.plot(time,Ch1[:,case])\n",
    "    plt.ylabel('Force')\n",
    "    plt.subplot(212)\n",
    "    plt.plot(time,Ch2[:,case],time,Ch3[:,case],time,Ch4[:,case],time,Ch5[:,case])\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Acceleration')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposing the channel dat: Ch(i) > X(i), i in {2,3,4,5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = Ch2.transpose()\n",
    "x3 = Ch3.transpose()\n",
    "x4 = Ch4.transpose()\n",
    "x5 = Ch5.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting AR model and building X1 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelOrder = 30\n",
    "\n",
    "x2r = []\n",
    "x3r = []\n",
    "x4r = []\n",
    "x5r = []\n",
    "\n",
    "for case in x2: #iterating over cases (observations #=850)\n",
    "    res = AutoReg(case, lags = modelOrder-1).fit()\n",
    "    x2r.append(res.params)\n",
    "for case in x3: #iterating over cases (observations #=850)\n",
    "    res = AutoReg(case, lags = modelOrder-1).fit()\n",
    "    x3r.append(res.params)\n",
    "for case in x4: #iterating over cases (observations #=850)\n",
    "    res = AutoReg(case, lags = modelOrder-1).fit()\n",
    "    x4r.append(res.params)\n",
    "for case in x5: #iterating over cases (observations #=850)\n",
    "    res = AutoReg(case, lags = modelOrder-1).fit()\n",
    "    x5r.append(res.params)\n",
    "\n",
    "X1 = np.concatenate([x2r, x3r, x4r, x5r], axis=1)\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nComponents = 0.95  # variance ratio to be explained or number of components if integer\n",
    "\n",
    "pca = PCA(n_components=nComponents).fit(X1)\n",
    "pcaVariances = pca.explained_variance_ratio_\n",
    "pcaComponents = [i for i in range(len(pcaVariances))]\n",
    "\n",
    "ax = sns.barplot(pcaComponents, pcaVariances*100, color='b')\n",
    "ax2 = ax.twinx()\n",
    "ax2 = sns.lineplot(pcaComponents, np.cumsum(pcaVariances)*100, color='r', marker='D')\n",
    "ax.tick_params(axis='y', colors='b')\n",
    "ax2.tick_params(axis='y', colors='r')\n",
    "\n",
    "ax.set_title('PCA: nc= ' + str(pcaVariances.shape[0]) + ', explained variance = ' + str(sum(pcaVariances))[:5])\n",
    "ax.set_xlabel('PCA coefficent')\n",
    "ax.set_ylabel('Explained Variance [%]')\n",
    "ax2.set_ylabel('Cumulative Explained Variance [%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building x2 by transforming the x1 data with the previous PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = pca.fit_transform(X1)\n",
    "X2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling features form both X1 and X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_scaled = MinMaxScaler(feature_range=(-1,1)).fit_transform(X1)\n",
    "X2_scaled = MinMaxScaler(feature_range=(-1,1)).fit_transform(X2)\n",
    "\n",
    "print (\"not scaled: \", X2[0], \"\\nscaled: \", X2_scaled[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating pandas dataframe versions of the scaled x1, x1+label, x2 and x2+label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx1 = pd.DataFrame(X1_scaled)\n",
    "dfx1_label = dfx1.copy()\n",
    "dfx1_label['label'] = y\n",
    "dfx1_label['stateCondition'] = y2\n",
    "\n",
    "dfx2 = pd.DataFrame(X2_scaled)\n",
    "dfx2_label = dfx2.copy()\n",
    "dfx2_label['label'] = y\n",
    "dfx2_label['stateCondition'] = y2\n",
    "\n",
    "dfx2_label.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR x PCA heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,8))\n",
    "plt.subplot(121)\n",
    "ax = sns.heatmap(dfx1)\n",
    "ax.set_title('Parameters of the AR model')\n",
    "ax.set_xlabel('coefficent')\n",
    "ax.set_ylabel('experiment')\n",
    "plt.subplot(122)\n",
    "ax2 = sns.heatmap(dfx2)\n",
    "ax2.set_title('Parameters of the PCA model')\n",
    "ax2.set_xlabel('coefficent')\n",
    "ax2.set_ylabel('experiment')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossplot of the first three features of the PCA analysis using labels as the 17 classes or states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = sns.pairplot(data=dfx2_label.iloc[:,[0,1,2,-2]], hue='label', palette='tab10')\n",
    "pp.fig.subplots_adjust(top=0.95, wspace=0.3)\n",
    "pp.fig.suptitle('First 3-PCA Components Pairwise Plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossplot of the first three features of the PCA analysis using the states undamaged (-1) and damaged (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = sns.pairplot(data=dfx2_label.iloc[:,[0,1,2,-1]], hue='stateCondition', palette='tab10')\n",
    "pp.fig.subplots_adjust(top=0.95, wspace=0.3)\n",
    "pp.fig.suptitle('First 3-PCA Components Pairwise Plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Multidimenssional Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(211)\n",
    "# pd.plotting.parallel_coordinates(dfx1_label, 'label')\n",
    "pd.plotting.parallel_coordinates(dfx1_label[dfx1_label['label']<=3].iloc[:,:-1], 'label')        #iloc[:,[0,1,2,-1]]\n",
    "plt.subplot(212)\n",
    "# pd.plotting.parallel_coordinates(dfx2_label, 'label')\n",
    "pd.plotting.parallel_coordinates(dfx2_label[dfx2_label['label']<=3].iloc[:,:-1], 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "Testing some values for the classes the comparison above suggests that it won't be easy to distinguish between classes 2 and 3 using the PCA features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Multidimenssional Spaces (damaged or undamaged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(211)\n",
    "# pd.plotting.parallel_coordinates(dfx1_label, 'label')\n",
    "pd.plotting.parallel_coordinates(dfx1_label.loc[:, dfx1_label.columns != 'label'], 'stateCondition', colormap=plt.get_cmap('Paired'))\n",
    "plt.subplot(212)\n",
    "# pd.plotting.parallel_coordinates(dfx2_label, 'label')\n",
    "pd.plotting.parallel_coordinates(dfx2_label.loc[:, dfx2_label.columns != 'label'], 'stateCondition', colormap=plt.get_cmap('Paired'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing labels to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escolher se modelos serão treinados com o label y = {1..17} ou y2 = {-1,1}\n",
    "yModelo = y\n",
    "# yModelo = y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data by separating train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From now on we dont use the '_scaled' for the test and train ensemples anymore\n",
    "\n",
    "test_size = 0.20\n",
    "X1_train, X1_test, y_train, y_test = train_test_split(X1_scaled, yModelo, test_size=test_size, random_state=10)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2_scaled, yModelo, test_size=test_size, random_state=10)\n",
    "\n",
    "print ('|Train X1|:', X1_train.shape[0])\n",
    "print ('|Test X1|:', X1_test.shape[0])\n",
    "print ('|Train y|:', len(y_train))\n",
    "print ('|Test y|:', len(y_test))\n",
    "\n",
    "# Check if the y split is the same in both calls of train_test_split\n",
    "print ('This must be zero: ', sum(y2_train-y_train))\n",
    "print ('This must also be zero: ', sum(y2_test-y_test))\n",
    "\n",
    "# type(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Linear Model (logistic regression with multiple classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LogisticRegression(multi_class='multinomial', max_iter=200).fit(X1_train, y_train)\n",
    "model2 = LogisticRegression(multi_class='multinomial', max_iter=200).fit(X2_train, y_train)\n",
    "\n",
    "yPred1_train = model1.predict(X1_train)\n",
    "yPred1_test = model1.predict(X1_test)\n",
    "\n",
    "yPred2_train = model2.predict(X2_train)\n",
    "yPred2_test = model2.predict(X2_test)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12, 6))\n",
    "x=np.arange(1,18,1);\n",
    "\n",
    "ax1 = sns.scatterplot(y_test,yPred1_test, s=50, ax=ax1, palette='colorblind');\n",
    "ax1.set_xlabel('True label');\n",
    "ax1.set_ylabel('Predicted label');\n",
    "ax1=sns.lineplot(x=x, y=x, ax=ax1);\n",
    "ax1.legend(['x=y (correct predictions)','PCA Softmax']);\n",
    "\n",
    "ax2 = sns.scatterplot(y_test,yPred2_test, s=50, ax=ax2);\n",
    "ax2.set_xlabel('True label');\n",
    "ax2.set_ylabel('Predicted label');\n",
    "ax2=sns.lineplot(x=x, y=x, ax=ax2);\n",
    "ax2.legend(['x=y (correct predictions)','Autoreg Softmax']);\n",
    "\n",
    "ax1.set_title('Softmax using Autoreg');\n",
    "ax2.set_title('Softmax using PCA (some mistaken points)');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "Although we have a good result in using PCA to reduce dimensionality to a few parameters, it seems like we lose a bit of information on the dataset. Indeed, the Auto Regression model performed better in classifying that dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing results in a 2x2 matrix\n",
    "dfResults = pd.DataFrame({\n",
    "        'Model': ['Autoreg', 'PCA'], \n",
    "        'Train': [accuracy_score(y_train, yPred1_train), accuracy_score(y_train, yPred2_train)], \n",
    "        'Test': [accuracy_score(y_test, yPred1_test), accuracy_score(y_test, yPred2_test)]})\n",
    "# unpivot dfResults in case of plotting graphs\n",
    "dfResults = dfResults.melt(id_vars='Model', value_name='Accuracy', var_name='Ensemble')\n",
    "dfResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
